{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xb8jr9QsUmr"
      },
      "source": [
        "# DETR Knowledge Distillation on KITTI Dataset\n",
        "\n",
        "This notebook demonstrates the complete pipeline for training a distilled DETR model on the KITTI dataset.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Setup**: Install dependencies and import libraries\n",
        "2. **Configuration**: Load from YAML file\n",
        "3. **Data Preparation**: Download and convert KITTI to COCO format\n",
        "4. **Dataset Loading**: Create PyTorch datasets\n",
        "5. **Model Setup**: Load teacher and student models\n",
        "6. **Training**: Train with knowledge distillation\n",
        "7. **Evaluation**: Evaluate with COCO metrics\n",
        "8. **Visualization**: Visualize predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e2EqMkNsUmt"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOr1O0mdsUmu",
        "outputId": "063db998-987c-44d4-a804-168ef973ea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MiQyyEmCsUmv"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers pycocotools pillow tqdm pyyaml matplotlib opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAP-qh4nsUmw",
        "outputId": "a9f681a9-f534-438e-d4e2-f5cd8fb2407b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'object-detection'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 61 (delta 14), reused 57 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 5.68 MiB | 11.32 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/object-detection\n",
            "Working directory: /content/object-detection\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!git clone https://github.com/HenryNVP/object-detection.git\n",
        "%cd object-detection\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4RJFmUwsUmx"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUtSx8vBsUmy",
        "outputId": "ef02956a-4210-46f6-851d-7d5cd093af2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.datasets.kitti_coco import build_kitti_coco_dataset, collate_fn\n",
        "from src.models import build_teacher_student_models\n",
        "from src.distillation import DistillationLoss, DistillationTrainer\n",
        "from src.utils import get_device, seed_all\n",
        "\n",
        "print(\"‚úì All imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt7TM98IsUmz"
      },
      "source": [
        "## 3. Load Configuration from YAML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 1000,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}"
      ],
      "metadata": {
        "id": "Kr8uCGbgsoJh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UPlxQyusUmz"
      },
      "source": [
        "## 4. Data Preparation\n",
        "\n",
        "Download KITTI dataset and convert to COCO format if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmGWoyfusUm0",
        "outputId": "2dc0ff25-1992-4ed8-8038-1755d9bcb245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KITTI Object Detection Dataset Downloader\n",
            "--------------------------------------------------\n",
            "Output directory: kitti_data\n",
            "--------------------------------------------------\n",
            "\n",
            "Downloading images...\n",
            "data_object_image_2.zip:   1% 152M/12.6G [00:07<10:15, 20.2MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/urllib/request.py\", line 268, in urlretrieve\n",
            "    while block := fp.read(bs):\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 100, in <module>\n",
            "    main()\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 82, in main\n",
            "    download_file(url, filepath)\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 37, in download_file\n",
            "    urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
            "  File \"/usr/lib/python3.12/urllib/request.py\", line 240, in urlretrieve\n",
            "    with contextlib.closing(urlopen(url, data)) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 360, in __exit__\n",
            "    self.thing.close()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 435, in close\n",
            "    self._close_conn()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 428, in _close_conn\n",
            "    fp.close()\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 783, in close\n",
            "    def close(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "\n",
            "Converting to COCO format...\n",
            "KITTI to COCO Converter\n",
            "--------------------------------------------------\n",
            "KITTI root: kitti_data/training\n",
            "Output dir: kitti_coco\n",
            "Train split: 0.8\n",
            "Random seed: 42\n",
            "--------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/object-detection/scripts/prepare_kitti_coco.py\", line 308, in <module>\n",
            "    main()\n",
            "  File \"/content/object-detection/scripts/prepare_kitti_coco.py\", line 230, in main\n",
            "    raise ValueError(f\"KITTI root not found: {args.kitti_root}\")\n",
            "ValueError: KITTI root not found: kitti_data/training\n",
            "\n",
            "‚úì Dataset ready!\n"
          ]
        }
      ],
      "source": [
        "# Download KITTI\n",
        "!python scripts/download_kitti.py --output-dir ./kitti_data\n",
        "\n",
        "# Convert to COCO format\n",
        "print(\"\\nConverting to COCO format...\")\n",
        "!python scripts/prepare_kitti_coco.py \\\n",
        "    --kitti-root {CONFIG['kitti_root']} \\\n",
        "    --output-dir {CONFIG['data_root']} \\\n",
        "    --train-split {CONFIG['train_split']} \\\n",
        "    --max-samples {CONFIG['max_samples']}\n",
        "\n",
        "print(\"\\n‚úì Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBEWBzKsUm1"
      },
      "source": [
        "## 5. Load Datasets and Create Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbSN0wdsUm1"
      },
      "outputs": [],
      "source": [
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Define transforms to convert PIL images to tensors\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def get_transform():\n",
        "    \"\"\"Basic transform to convert PIL images to tensors.\"\"\"\n",
        "    return T.Compose([\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "# Note: We'll handle DETR-specific preprocessing in the trainer\n",
        "train_dataset = build_kitti_coco_dataset(\n",
        "    split='train',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,  # We'll use image_processor in trainer\n",
        ")\n",
        "\n",
        "val_dataset = build_kitti_coco_dataset(\n",
        "    split='val',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,  # We'll use image_processor in trainer\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train dataset: {len(train_dataset)} samples ({len(train_loader)} batches)\")\n",
        "print(f\"‚úì Val dataset: {len(val_dataset)} samples ({len(val_loader)} batches)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLt3o9-sUm2"
      },
      "source": [
        "## 6. Load Teacher and Student Models\n",
        "\n",
        "**Note**: Using `facebook/detr-resnet-50` (official Facebook DETR model) as teacher.  \n",
        "Creating a custom smaller student model for distillation with:\n",
        "- **Smaller backbone**: ResNet-18 vs ResNet-50 (~11M vs ~25M params)\n",
        "- **Fewer transformer layers**: 4 vs 6 (both encoder and decoder)\n",
        "- **Fewer attention heads**: 4 vs 8\n",
        "- **Smaller FFN dimension**: 1024 vs 2048\n",
        "- **Result**: ~60-70% parameter reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd7LFD8IsUm3"
      },
      "outputs": [],
      "source": [
        "from transformers import DetrForObjectDetection, DetrImageProcessor, DetrConfig\n",
        "from torchvision.models import resnet18, resnet50\n",
        "import torch.nn as nn\n",
        "\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# Use facebook/detr-resnet-50 (available on HuggingFace)\n",
        "teacher_model_name = \"facebook/detr-resnet-50\"\n",
        "print(f\"Teacher: {teacher_model_name} (ResNet-50 backbone)\")\n",
        "\n",
        "# Load teacher model and image processor\n",
        "image_processor = DetrImageProcessor.from_pretrained(teacher_model_name)\n",
        "teacher_model = DetrForObjectDetection.from_pretrained(\n",
        "    teacher_model_name,\n",
        "    num_labels=CONFIG['num_labels'],\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# Freeze teacher\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"‚úì Teacher model loaded and frozen\")\n",
        "\n",
        "# Create smaller student model with ResNet-18 backbone\n",
        "print(\"\\nCreating smaller student model with ResNet-18 backbone...\")\n",
        "\n",
        "# Load ResNet-18 and remove classification head\n",
        "resnet18_backbone = resnet18(pretrained=True)\n",
        "# Remove avgpool and fc layers, keep only conv layers\n",
        "student_backbone = nn.Sequential(*list(resnet18_backbone.children())[:-2])\n",
        "\n",
        "# Create DETR config for student with smaller dimensions\n",
        "config_detr = DetrConfig.from_pretrained(teacher_model_name)\n",
        "config_detr.num_labels = CONFIG['num_labels']\n",
        "\n",
        "# Adjust for ResNet-18's smaller feature dimension\n",
        "# ResNet-18 outputs 512 channels (vs 2048 for ResNet-50)\n",
        "config_detr.d_model = 256  # Keep hidden dimension same\n",
        "config_detr.encoder_attention_heads = 4  # Fewer attention heads (default 8)\n",
        "config_detr.decoder_attention_heads = 4\n",
        "config_detr.encoder_layers = 4  # Fewer encoder layers (default 6)\n",
        "config_detr.decoder_layers = 4  # Fewer decoder layers (default 6)\n",
        "config_detr.encoder_ffn_dim = 1024  # Smaller FFN (default 2048)\n",
        "config_detr.decoder_ffn_dim = 1024\n",
        "\n",
        "# Create student model (will use default ResNet-50 backbone initially)\n",
        "student_model = DetrForObjectDetection(config_detr)\n",
        "\n",
        "# Replace the backbone with ResNet-18\n",
        "print(\"  ‚Üí Replacing backbone with ResNet-18...\")\n",
        "student_model.model.backbone.conv_encoder.model = student_backbone\n",
        "\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "print(\"‚úì Student model created with ResNet-18 backbone\")\n",
        "\n",
        "# Count parameters\n",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "student_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"  Teacher parameters: {teacher_params:,} (ResNet-50 backbone)\")\n",
        "print(f\"  Student parameters: {student_params:,} (ResNet-18 backbone, {student_trainable:,} trainable)\")\n",
        "print(f\"  Compression ratio: {student_params / teacher_params:.2%}\")\n",
        "print(f\"  Size reduction: {(1 - student_params / teacher_params):.1%}\")\n",
        "print(f\"\\nBackbone comparison:\")\n",
        "print(f\"  Teacher backbone: ResNet-50 (~25M params)\")\n",
        "print(f\"  Student backbone: ResNet-18 (~11M params)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7S9IF6lsUm3"
      },
      "source": [
        "## 7. Setup Training with Distillation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN2ed2DisUm4"
      },
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    student_model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        ")\n",
        "\n",
        "# Setup distillation loss\n",
        "distillation_loss = DistillationLoss(\n",
        "    temperature=CONFIG['temperature'],\n",
        "    alpha=CONFIG['alpha'],\n",
        ")\n",
        "\n",
        "# Create trainer with image_processor for PIL image handling\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    distillation_loss=distillation_loss,\n",
        "    device=device,\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    image_processor=image_processor,  # Pass processor to handle PIL images\n",
        ")\n",
        "\n",
        "print(\"‚úì Training setup complete\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yIJldTZsUm4"
      },
      "source": [
        "## 8. Train Model\n",
        "\n",
        "Train the student model with knowledge distillation from the teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ficUfL7MsUm4"
      },
      "outputs": [],
      "source": [
        "print(f\"Starting training for {CONFIG['epochs']} epochs...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Train\n",
        "trainer.train(num_epochs=CONFIG['epochs'], save_every=1)\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n",
        "print(f\"Checkpoints saved to: {CONFIG['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3SDC8ECsUm5"
      },
      "source": [
        "## 9. Evaluate Model\n",
        "\n",
        "Evaluate the trained student model on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m0gPOTOsUm5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        # Process images with image_processor\n",
        "        pixel_values = torch.stack(images)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "\n",
        "        for i, target in enumerate(targets):\n",
        "            image_id = target['image_id'].item()\n",
        "            logits = outputs.logits[i]\n",
        "            boxes = outputs.pred_boxes[i]\n",
        "\n",
        "            # Get predicted class and score\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "\n",
        "            # Filter low confidence predictions\n",
        "            keep = scores > 0.3\n",
        "            for box, score, label in zip(boxes[keep], scores[keep], labels[keep]):\n",
        "                # Convert from normalized [cx, cy, w, h] to COCO [x, y, w, h]\n",
        "                cx, cy, w, h = box.cpu().tolist()\n",
        "                img_h, img_w = target['orig_size'].tolist()\n",
        "                x = (cx - w/2) * img_w\n",
        "                y = (cy - h/2) * img_h\n",
        "                w = w * img_w\n",
        "                h = h * img_h\n",
        "\n",
        "                predictions.append({\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': int(label.item()) + 1,\n",
        "                    'bbox': [x, y, w, h],\n",
        "                    'score': float(score.item()),\n",
        "                })\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "predictions = evaluate_model(student_model, val_loader, device)\n",
        "print(f\"‚úì Generated {len(predictions)} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gGXLb2ZsUm5"
      },
      "outputs": [],
      "source": [
        "# Run COCO evaluation\n",
        "if len(predictions) > 0:\n",
        "    print(\"\\nRunning COCO evaluation...\")\n",
        "    ann_file = Path(CONFIG['data_root']) / 'annotations' / 'instances_val.json'\n",
        "\n",
        "    coco_gt = COCO(str(ann_file))\n",
        "    coco_dt = coco_gt.loadRes(predictions)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No predictions to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClUk29bDsUm6"
      },
      "source": [
        "## 10. Visualize Predictions\n",
        "\n",
        "Visualize sample predictions from the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dgZnAEosUm6"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, device, num_samples=3):\n",
        "    \"\"\"Visualize predictions on random samples.\"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    if num_samples == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, ax in zip(indices, axes):\n",
        "            image, target = dataset[idx]\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "            outputs = model(pixel_values=image_tensor)\n",
        "\n",
        "            logits = outputs.logits[0]\n",
        "            boxes = outputs.pred_boxes[0]\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "            keep = scores > 0.5\n",
        "\n",
        "            # Convert image to numpy for visualization\n",
        "            img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = (img_np * 255).astype(np.uint8)\n",
        "            h, w = img_np.shape[:2]\n",
        "\n",
        "            # Draw bounding boxes\n",
        "            for box, score in zip(boxes[keep], scores[keep]):\n",
        "                cx, cy, bw, bh = box.cpu().numpy()\n",
        "                x1 = int((cx - bw/2) * w)\n",
        "                y1 = int((cy - bh/2) * h)\n",
        "                x2 = int((cx + bw/2) * w)\n",
        "                y2 = int((cy + bh/2) * h)\n",
        "                cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(img_np, f\"{score.item():.2f}\", (x1, y1-5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            ax.imshow(img_np)\n",
        "            ax.set_title(f\"Predictions (n={keep.sum()})\")\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(student_model, val_dataset, device, num_samples=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVcPDa4QsUm6"
      },
      "source": [
        "## 11. Summary\n",
        "\n",
        "Knowledge distillation training completed successfully!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I1-ZOX7sUm7"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üéâ KNOWLEDGE DISTILLATION PIPELINE COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úì Configuration loaded from YAML\")\n",
        "print(\"‚úì Dataset prepared and loaded\")\n",
        "print(\"‚úì Teacher and student models configured\")\n",
        "print(\"‚úì Training completed with distillation\")\n",
        "print(\"‚úì Model evaluated with COCO metrics\")\n",
        "print(\"‚úì Predictions visualized\")\n",
        "print(f\"\\nüìÅ Output directory: {CONFIG['output_dir']}\")\n",
        "print(\"   - best.pth: Best model checkpoint\")\n",
        "print(\"   - epoch_*.pth: Epoch checkpoints\")\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"   1. Train for more epochs (edit YAML config)\")\n",
        "print(\"   2. Tune hyperparameters in YAML\")\n",
        "print(\"   3. Try different model pairs\")\n",
        "print(\"   4. Deploy the model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJLunDFdsUm7"
      },
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Create flattened CONFIG for easier access\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 200,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration loaded from:\", config_path)\n",
        "print(\"\\nüîß Notebook overrides (for faster demo):\")\n",
        "print(f\"  ‚Ä¢ epochs: {config['training']['epochs']} ‚Üí {CONFIG['epochs']}\")\n",
        "print(f\"  ‚Ä¢ max_samples: full dataset ‚Üí {CONFIG['max_samples']}\")\n",
        "print(f\"  ‚Ä¢ output_dir: {config['output_dir']} ‚Üí {CONFIG['output_dir']}\")\n",
        "\n",
        "seed_all(CONFIG['seed'])\n",
        "device = get_device(CONFIG['device'])\n",
        "print(f\"\\nüñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nüìä Active Configuration:\")\n",
        "print(f\"  Teacher: {CONFIG['teacher_model']}\")\n",
        "print(f\"  Student: {CONFIG['student_model']}\")\n",
        "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}