{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xb8jr9QsUmr"
      },
      "source": [
        "# DETR Knowledge Distillation on KITTI Dataset\n",
        "\n",
        "This notebook demonstrates the complete pipeline for training a distilled DETR model on the KITTI dataset.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Setup**: Install dependencies and import libraries\n",
        "2. **Configuration**: Load from YAML file\n",
        "3. **Data Preparation**: Download and convert KITTI to COCO format\n",
        "4. **Dataset Loading**: Create PyTorch datasets\n",
        "5. **Model Setup**: Load teacher and student models\n",
        "6. **Training**: Train with knowledge distillation\n",
        "7. **Evaluation**: Evaluate with COCO metrics\n",
        "8. **Visualization**: Visualize predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e2EqMkNsUmt"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOr1O0mdsUmu",
        "outputId": "063db998-987c-44d4-a804-168ef973ea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MiQyyEmCsUmv"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers pycocotools pillow tqdm pyyaml matplotlib opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAP-qh4nsUmw",
        "outputId": "a9f681a9-f534-438e-d4e2-f5cd8fb2407b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'object-detection'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 61 (delta 14), reused 57 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 5.68 MiB | 11.32 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/object-detection\n",
            "Working directory: /content/object-detection\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!git clone https://github.com/HenryNVP/object-detection.git\n",
        "%cd object-detection\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4RJFmUwsUmx"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUtSx8vBsUmy",
        "outputId": "ef02956a-4210-46f6-851d-7d5cd093af2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 All imports successful\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.datasets.kitti_coco import build_kitti_coco_dataset, collate_fn\n",
        "from src.models import build_teacher_student_models\n",
        "from src.distillation import DistillationLoss, DistillationTrainer\n",
        "from src.utils import get_device, seed_all\n",
        "\n",
        "print(\"\u2713 All imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt7TM98IsUmz"
      },
      "source": [
        "## 3. Load Configuration from YAML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 1000,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}"
      ],
      "metadata": {
        "id": "Kr8uCGbgsoJh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UPlxQyusUmz"
      },
      "source": [
        "## 4. Data Preparation\n",
        "\n",
        "Download KITTI dataset and convert to COCO format if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmGWoyfusUm0",
        "outputId": "2dc0ff25-1992-4ed8-8038-1755d9bcb245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KITTI Object Detection Dataset Downloader\n",
            "--------------------------------------------------\n",
            "Output directory: kitti_data\n",
            "--------------------------------------------------\n",
            "\n",
            "Downloading images...\n",
            "data_object_image_2.zip:   1% 152M/12.6G [00:07<10:15, 20.2MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/urllib/request.py\", line 268, in urlretrieve\n",
            "    while block := fp.read(bs):\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 100, in <module>\n",
            "    main()\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 82, in main\n",
            "    download_file(url, filepath)\n",
            "  File \"/content/object-detection/scripts/download_kitti.py\", line 37, in download_file\n",
            "    urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
            "  File \"/usr/lib/python3.12/urllib/request.py\", line 240, in urlretrieve\n",
            "    with contextlib.closing(urlopen(url, data)) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 360, in __exit__\n",
            "    self.thing.close()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 435, in close\n",
            "    self._close_conn()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 428, in _close_conn\n",
            "    fp.close()\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 783, in close\n",
            "    def close(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "\n",
            "Converting to COCO format...\n",
            "KITTI to COCO Converter\n",
            "--------------------------------------------------\n",
            "KITTI root: kitti_data/training\n",
            "Output dir: kitti_coco\n",
            "Train split: 0.8\n",
            "Random seed: 42\n",
            "--------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/object-detection/scripts/prepare_kitti_coco.py\", line 308, in <module>\n",
            "    main()\n",
            "  File \"/content/object-detection/scripts/prepare_kitti_coco.py\", line 230, in main\n",
            "    raise ValueError(f\"KITTI root not found: {args.kitti_root}\")\n",
            "ValueError: KITTI root not found: kitti_data/training\n",
            "\n",
            "\u2713 Dataset ready!\n"
          ]
        }
      ],
      "source": [
        "# Download KITTI\n",
        "!python scripts/download_kitti.py --output-dir ./kitti_data\n",
        "\n",
        "# Convert to COCO format\n",
        "print(\"\\nConverting to COCO format...\")\n",
        "!python scripts/prepare_kitti_coco.py \\\n",
        "    --kitti-root {CONFIG['kitti_root']} \\\n",
        "    --output-dir {CONFIG['data_root']} \\\n",
        "    --train-split {CONFIG['train_split']} \\\n",
        "    --max-samples {CONFIG['max_samples']}\n",
        "\n",
        "print(\"\\n\u2713 Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBEWBzKsUm1"
      },
      "source": [
        "## 5. Load Datasets and Create Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKbSN0wdsUm1"
      },
      "outputs": [],
      "source": [
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Define transforms to convert PIL images to tensors\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def get_transform():\n",
        "    \"\"\"Basic transform to convert PIL images to tensors.\"\"\"\n",
        "    return T.Compose([\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "\n",
        "# Note: We'll handle DETR-specific preprocessing in the trainer\n",
        "train_dataset = build_kitti_coco_dataset(\n",
        "    split='train',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,  # We'll use image_processor in trainer\n",
        ")\n",
        "\n",
        "val_dataset = build_kitti_coco_dataset(\n",
        "    split='val',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,  # We'll use image_processor in trainer\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"\u2713 Train dataset: {len(train_dataset)} samples ({len(train_loader)} batches)\")\n",
        "print(f\"\u2713 Val dataset: {len(val_dataset)} samples ({len(val_loader)} batches)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLt3o9-sUm2"
      },
      "source": [
        "## 6. Load Teacher and Student Models",
        "",
        "**Note**: Using `facebook/detr-resnet-50` (official Facebook DETR model) as teacher.  ",
        "Creating a smaller student model for distillation by:",
        "- **Same backbone**: ResNet-50 (to avoid channel mismatch issues)",
        "- **Fewer transformer layers**: 3 vs 6 (both encoder and decoder)",
        "- **Fewer attention heads**: 4 vs 8",
        "- **Smaller FFN dimension**: 1024 vs 2048",
        "- **Result**: ~30-40% parameter reduction while maintaining compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd7LFD8IsUm3"
      },
      "outputs": [],
      "source": [
        "from transformers import DetrForObjectDetection, DetrImageProcessor, DetrConfig",
        "import torch.nn as nn",
        "",
        "print(\"Loading models...\")",
        "",
        "# Use facebook/detr-resnet-50 (available on HuggingFace)",
        "teacher_model_name = \"facebook/detr-resnet-50\"",
        "print(f\"Teacher: {teacher_model_name} (ResNet-50 backbone)\")",
        "",
        "# Load teacher model and image processor",
        "image_processor = DetrImageProcessor.from_pretrained(teacher_model_name)",
        "teacher_model = DetrForObjectDetection.from_pretrained(",
        "    teacher_model_name,",
        "    num_labels=CONFIG['num_labels'],",
        "    ignore_mismatched_sizes=True",
        ")",
        "teacher_model = teacher_model.to(device)",
        "teacher_model.eval()",
        "",
        "# Freeze teacher",
        "for param in teacher_model.parameters():",
        "    param.requires_grad = False",
        "",
        "print(\"\u2713 Teacher model loaded and frozen\")",
        "",
        "# Create smaller student model",
        "print(\"\\nCreating smaller student model...\")",
        "print(\"  \u2192 Using same backbone as teacher (ResNet-50)\")",
        "print(\"  \u2192 Reducing transformer dimensions for compression\")",
        "",
        "# Create DETR config for student with smaller dimensions",
        "config_detr = DetrConfig.from_pretrained(teacher_model_name)",
        "config_detr.num_labels = CONFIG['num_labels']",
        "",
        "# Reduce transformer size for student (keep backbone same to avoid channel issues)",
        "config_detr.d_model = 256  # Hidden dimension (default: 256, already small)",
        "config_detr.encoder_attention_heads = 4  # Fewer attention heads (default 8)",
        "config_detr.decoder_attention_heads = 4",
        "config_detr.encoder_layers = 3  # Fewer encoder layers (default 6)",
        "config_detr.decoder_layers = 3  # Fewer decoder layers (default 6)",
        "config_detr.encoder_ffn_dim = 1024  # Smaller FFN (default 2048)",
        "config_detr.decoder_ffn_dim = 1024",
        "",
        "# Create student model with reduced transformer",
        "student_model = DetrForObjectDetection(config_detr)",
        "student_model = student_model.to(device)",
        "",
        "print(\"\u2713 Student model created with smaller transformer\")",
        "",
        "# Count parameters",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())",
        "student_params = sum(p.numel() for p in student_model.parameters())",
        "student_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)",
        "",
        "print(f\"\\n\ud83d\udcca Model Statistics:\")",
        "print(f\"  Teacher parameters: {teacher_params:,}\")",
        "print(f\"  Student parameters: {student_params:,} ({student_trainable:,} trainable)\")",
        "print(f\"  Compression ratio: {student_params / teacher_params:.2%}\")",
        "print(f\"  Size reduction: {(1 - student_params / teacher_params):.1%}\")",
        "print(f\"\\nArchitecture comparison:\")",
        "print(f\"  Teacher: ResNet-50 + 6 encoder/6 decoder layers + 8 heads + FFN 2048\")",
        "print(f\"  Student: ResNet-50 + 3 encoder/3 decoder layers + 4 heads + FFN 1024\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7S9IF6lsUm3"
      },
      "source": [
        "## 7. Setup Training with Distillation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN2ed2DisUm4"
      },
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    student_model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        ")\n",
        "\n",
        "# Setup distillation loss\n",
        "distillation_loss = DistillationLoss(\n",
        "    temperature=CONFIG['temperature'],\n",
        "    alpha=CONFIG['alpha'],\n",
        ")\n",
        "\n",
        "# Create trainer with image_processor for PIL image handling\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    distillation_loss=distillation_loss,\n",
        "    device=device,\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    image_processor=image_processor,  # Pass processor to handle PIL images\n",
        ")\n",
        "\n",
        "print(\"\u2713 Training setup complete\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yIJldTZsUm4"
      },
      "source": [
        "## 8. Train Model\n",
        "\n",
        "Train the student model with knowledge distillation from the teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ficUfL7MsUm4"
      },
      "outputs": [],
      "source": [
        "print(f\"Starting training for {CONFIG['epochs']} epochs...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Train\n",
        "trainer.train(num_epochs=CONFIG['epochs'], save_every=1)\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "print(f\"Checkpoints saved to: {CONFIG['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3SDC8ECsUm5"
      },
      "source": [
        "## 9. Evaluate Model\n",
        "\n",
        "Evaluate the trained student model on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m0gPOTOsUm5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        # Process images with image_processor\n",
        "        pixel_values = torch.stack(images)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "\n",
        "        for i, target in enumerate(targets):\n",
        "            image_id = target['image_id'].item()\n",
        "            logits = outputs.logits[i]\n",
        "            boxes = outputs.pred_boxes[i]\n",
        "\n",
        "            # Get predicted class and score\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "\n",
        "            # Filter low confidence predictions\n",
        "            keep = scores > 0.3\n",
        "            for box, score, label in zip(boxes[keep], scores[keep], labels[keep]):\n",
        "                # Convert from normalized [cx, cy, w, h] to COCO [x, y, w, h]\n",
        "                cx, cy, w, h = box.cpu().tolist()\n",
        "                img_h, img_w = target['orig_size'].tolist()\n",
        "                x = (cx - w/2) * img_w\n",
        "                y = (cy - h/2) * img_h\n",
        "                w = w * img_w\n",
        "                h = h * img_h\n",
        "\n",
        "                predictions.append({\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': int(label.item()) + 1,\n",
        "                    'bbox': [x, y, w, h],\n",
        "                    'score': float(score.item()),\n",
        "                })\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "predictions = evaluate_model(student_model, val_loader, device)\n",
        "print(f\"\u2713 Generated {len(predictions)} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gGXLb2ZsUm5"
      },
      "outputs": [],
      "source": [
        "# Run COCO evaluation\n",
        "if len(predictions) > 0:\n",
        "    print(\"\\nRunning COCO evaluation...\")\n",
        "    ann_file = Path(CONFIG['data_root']) / 'annotations' / 'instances_val.json'\n",
        "\n",
        "    coco_gt = COCO(str(ann_file))\n",
        "    coco_dt = coco_gt.loadRes(predictions)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f No predictions to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClUk29bDsUm6"
      },
      "source": [
        "## 10. Visualize Predictions\n",
        "\n",
        "Visualize sample predictions from the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dgZnAEosUm6"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, device, num_samples=3):\n",
        "    \"\"\"Visualize predictions on random samples.\"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    if num_samples == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, ax in zip(indices, axes):\n",
        "            image, target = dataset[idx]\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "            outputs = model(pixel_values=image_tensor)\n",
        "\n",
        "            logits = outputs.logits[0]\n",
        "            boxes = outputs.pred_boxes[0]\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "            keep = scores > 0.5\n",
        "\n",
        "            # Convert image to numpy for visualization\n",
        "            img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = (img_np * 255).astype(np.uint8)\n",
        "            h, w = img_np.shape[:2]\n",
        "\n",
        "            # Draw bounding boxes\n",
        "            for box, score in zip(boxes[keep], scores[keep]):\n",
        "                cx, cy, bw, bh = box.cpu().numpy()\n",
        "                x1 = int((cx - bw/2) * w)\n",
        "                y1 = int((cy - bh/2) * h)\n",
        "                x2 = int((cx + bw/2) * w)\n",
        "                y2 = int((cy + bh/2) * h)\n",
        "                cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(img_np, f\"{score.item():.2f}\", (x1, y1-5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            ax.imshow(img_np)\n",
        "            ax.set_title(f\"Predictions (n={keep.sum()})\")\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(student_model, val_dataset, device, num_samples=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVcPDa4QsUm6"
      },
      "source": [
        "## 11. Summary\n",
        "\n",
        "Knowledge distillation training completed successfully!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I1-ZOX7sUm7"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udf89 KNOWLEDGE DISTILLATION PIPELINE COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n\u2713 Configuration loaded from YAML\")\n",
        "print(\"\u2713 Dataset prepared and loaded\")\n",
        "print(\"\u2713 Teacher and student models configured\")\n",
        "print(\"\u2713 Training completed with distillation\")\n",
        "print(\"\u2713 Model evaluated with COCO metrics\")\n",
        "print(\"\u2713 Predictions visualized\")\n",
        "print(f\"\\n\ud83d\udcc1 Output directory: {CONFIG['output_dir']}\")\n",
        "print(\"   - best.pth: Best model checkpoint\")\n",
        "print(\"   - epoch_*.pth: Epoch checkpoints\")\n",
        "print(\"\\n\ud83d\ude80 Next Steps:\")\n",
        "print(\"   1. Train for more epochs (edit YAML config)\")\n",
        "print(\"   2. Tune hyperparameters in YAML\")\n",
        "print(\"   3. Try different model pairs\")\n",
        "print(\"   4. Deploy the model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJLunDFdsUm7"
      },
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Create flattened CONFIG for easier access\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 200,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}\n",
        "\n",
        "print(\"\ud83d\udccb Configuration loaded from:\", config_path)\n",
        "print(\"\\n\ud83d\udd27 Notebook overrides (for faster demo):\")\n",
        "print(f\"  \u2022 epochs: {config['training']['epochs']} \u2192 {CONFIG['epochs']}\")\n",
        "print(f\"  \u2022 max_samples: full dataset \u2192 {CONFIG['max_samples']}\")\n",
        "print(f\"  \u2022 output_dir: {config['output_dir']} \u2192 {CONFIG['output_dir']}\")\n",
        "\n",
        "seed_all(CONFIG['seed'])\n",
        "device = get_device(CONFIG['device'])\n",
        "print(f\"\\n\ud83d\udda5\ufe0f  Using device: {device}\")\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Active Configuration:\")\n",
        "print(f\"  Teacher: {CONFIG['teacher_model']}\")\n",
        "print(f\"  Student: {CONFIG['student_model']}\")\n",
        "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}