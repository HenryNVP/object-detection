{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DETR Knowledge Distillation on KITTI Dataset\n",
        "\n",
        "This notebook demonstrates the complete pipeline for training a distilled DETR model on the KITTI dataset.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Setup**: Install dependencies and import libraries\n",
        "2. **Configuration**: Load from YAML file\n",
        "3. **Data Preparation**: Download and convert KITTI to COCO format\n",
        "4. **Dataset Loading**: Create PyTorch datasets\n",
        "5. **Model Setup**: Load teacher and student models\n",
        "6. **Training**: Train with knowledge distillation\n",
        "7. **Evaluation**: Evaluate with COCO metrics\n",
        "8. **Visualization**: Visualize predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers pycocotools pillow tqdm pyyaml matplotlib opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "!git clone https://github.com/HenryNVP/object-detection.git\n",
        "%cd object-detection\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.datasets import build_kitti_coco_dataset, collate_fn\n",
        "from src.models import build_teacher_student_models\n",
        "from src.distillation import DistillationLoss, DistillationTrainer\n",
        "from src.utils import get_device, seed_all\n",
        "\n",
        "print(\"✓ All imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {get_device()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Configuration from YAML\n",
        "\n",
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 1000,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preparation\n",
        "\n",
        "Download KITTI dataset and convert to COCO format if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download KITTI\n",
        "!python scripts/download_kitti.py --output-dir ./kitti_data\n",
        "\n",
        "# Convert to COCO format\n",
        "print(\"\\nConverting to COCO format...\")\n",
        "!python scripts/prepare_kitti_coco.py \\\n",
        "    --kitti-root {CONFIG['kitti_root']} \\\n",
        "    --output-dir {CONFIG['data_root']} \\\n",
        "    --train-split {CONFIG['train_split']} \\\n",
        "    --max-samples {CONFIG['max_samples']}\n",
        "\n",
        "print(\"\\n✓ Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Datasets and Create Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading datasets...\")\n",
        "\n",
        "train_dataset = build_kitti_coco_dataset(\n",
        "    split='train',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,\n",
        ")\n",
        "\n",
        "val_dataset = build_kitti_coco_dataset(\n",
        "    split='val',\n",
        "    data_root=CONFIG['data_root'],\n",
        "    transforms=None,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} samples ({len(train_loader)} batches)\")\n",
        "print(f\"✓ Val dataset: {len(val_dataset)} samples ({len(val_loader)} batches)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Teacher and Student Models\n",
        "\n",
        "**Note**: Using `facebook/detr-resnet-50` (official Facebook DETR model) as teacher.  \n",
        "Creating a custom smaller student model for distillation with:\n",
        "- **Smaller backbone**: ResNet-18 vs ResNet-50 (~11M vs ~25M params)\n",
        "- **Fewer transformer layers**: 4 vs 6 (both encoder and decoder)\n",
        "- **Fewer attention heads**: 4 vs 8\n",
        "- **Smaller FFN dimension**: 1024 vs 2048\n",
        "- **Result**: ~60-70% parameter reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DetrForObjectDetection, DetrImageProcessor, DetrConfig\n",
        "from torchvision.models import resnet18, resnet50\n",
        "import torch.nn as nn\n",
        "\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# Use facebook/detr-resnet-50 (available on HuggingFace)\n",
        "teacher_model_name = \"facebook/detr-resnet-50\"\n",
        "print(f\"Teacher: {teacher_model_name} (ResNet-50 backbone)\")\n",
        "\n",
        "# Load teacher model and image processor\n",
        "image_processor = DetrImageProcessor.from_pretrained(teacher_model_name)\n",
        "teacher_model = DetrForObjectDetection.from_pretrained(\n",
        "    teacher_model_name,\n",
        "    num_labels=CONFIG['num_labels'],\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# Freeze teacher\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"✓ Teacher model loaded and frozen\")\n",
        "\n",
        "# Create smaller student model with ResNet-18 backbone\n",
        "print(\"\\nCreating smaller student model with ResNet-18 backbone...\")\n",
        "\n",
        "# Load ResNet-18 and remove classification head\n",
        "resnet18_backbone = resnet18(pretrained=True)\n",
        "# Remove avgpool and fc layers, keep only conv layers\n",
        "student_backbone = nn.Sequential(*list(resnet18_backbone.children())[:-2])\n",
        "\n",
        "# Create DETR config for student with smaller dimensions\n",
        "config_detr = DetrConfig.from_pretrained(teacher_model_name)\n",
        "config_detr.num_labels = CONFIG['num_labels']\n",
        "\n",
        "# Adjust for ResNet-18's smaller feature dimension\n",
        "# ResNet-18 outputs 512 channels (vs 2048 for ResNet-50)\n",
        "config_detr.d_model = 256  # Keep hidden dimension same\n",
        "config_detr.encoder_attention_heads = 4  # Fewer attention heads (default 8)\n",
        "config_detr.decoder_attention_heads = 4\n",
        "config_detr.encoder_layers = 4  # Fewer encoder layers (default 6)\n",
        "config_detr.decoder_layers = 4  # Fewer decoder layers (default 6)\n",
        "config_detr.encoder_ffn_dim = 1024  # Smaller FFN (default 2048)\n",
        "config_detr.decoder_ffn_dim = 1024\n",
        "\n",
        "# Create student model (will use default ResNet-50 backbone initially)\n",
        "student_model = DetrForObjectDetection(config_detr)\n",
        "\n",
        "# Replace the backbone with ResNet-18\n",
        "print(\"  → Replacing backbone with ResNet-18...\")\n",
        "student_model.model.backbone.conv_encoder.model = student_backbone\n",
        "\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "print(\"✓ Student model created with ResNet-18 backbone\")\n",
        "\n",
        "# Count parameters\n",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
        "student_params = sum(p.numel() for p in student_model.parameters())\n",
        "student_trainable = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n📊 Model Statistics:\")\n",
        "print(f\"  Teacher parameters: {teacher_params:,} (ResNet-50 backbone)\")\n",
        "print(f\"  Student parameters: {student_params:,} (ResNet-18 backbone, {student_trainable:,} trainable)\")\n",
        "print(f\"  Compression ratio: {student_params / teacher_params:.2%}\")\n",
        "print(f\"  Size reduction: {(1 - student_params / teacher_params):.1%}\")\n",
        "print(f\"\\nBackbone comparison:\")\n",
        "print(f\"  Teacher backbone: ResNet-50 (~25M params)\")\n",
        "print(f\"  Student backbone: ResNet-18 (~11M params)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Setup Training with Distillation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    student_model.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        ")\n",
        "\n",
        "# Setup distillation loss\n",
        "distillation_loss = DistillationLoss(\n",
        "    temperature=CONFIG['temperature'],\n",
        "    alpha=CONFIG['alpha'],\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    distillation_loss=distillation_loss,\n",
        "    device=device,\n",
        "    output_dir=CONFIG['output_dir'],\n",
        ")\n",
        "\n",
        "print(\"✓ Training setup complete\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model\n",
        "\n",
        "Train the student model with knowledge distillation from the teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Starting training for {CONFIG['epochs']} epochs...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Train\n",
        "trainer.train(num_epochs=CONFIG['epochs'], save_every=1)\n",
        "\n",
        "print(\"\\n✓ Training complete!\")\n",
        "print(f\"Checkpoints saved to: {CONFIG['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluate Model\n",
        "\n",
        "Evaluate the trained student model on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    \n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        \n",
        "        # Process images with image_processor\n",
        "        pixel_values = torch.stack(images)\n",
        "        outputs = model(pixel_values=pixel_values)\n",
        "        \n",
        "        for i, target in enumerate(targets):\n",
        "            image_id = target['image_id'].item()\n",
        "            logits = outputs.logits[i]\n",
        "            boxes = outputs.pred_boxes[i]\n",
        "            \n",
        "            # Get predicted class and score\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "            \n",
        "            # Filter low confidence predictions\n",
        "            keep = scores > 0.3\n",
        "            for box, score, label in zip(boxes[keep], scores[keep], labels[keep]):\n",
        "                # Convert from normalized [cx, cy, w, h] to COCO [x, y, w, h]\n",
        "                cx, cy, w, h = box.cpu().tolist()\n",
        "                img_h, img_w = target['orig_size'].tolist()\n",
        "                x = (cx - w/2) * img_w\n",
        "                y = (cy - h/2) * img_h\n",
        "                w = w * img_w\n",
        "                h = h * img_h\n",
        "                \n",
        "                predictions.append({\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': int(label.item()) + 1,\n",
        "                    'bbox': [x, y, w, h],\n",
        "                    'score': float(score.item()),\n",
        "                })\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "predictions = evaluate_model(student_model, val_loader, device)\n",
        "print(f\"✓ Generated {len(predictions)} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run COCO evaluation\n",
        "if len(predictions) > 0:\n",
        "    print(\"\\nRunning COCO evaluation...\")\n",
        "    ann_file = Path(CONFIG['data_root']) / 'annotations' / 'instances_val.json'\n",
        "    \n",
        "    coco_gt = COCO(str(ann_file))\n",
        "    coco_dt = coco_gt.loadRes(predictions)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "else:\n",
        "    print(\"\\n⚠️ No predictions to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize Predictions\n",
        "\n",
        "Visualize sample predictions from the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, device, num_samples=3):\n",
        "    \"\"\"Visualize predictions on random samples.\"\"\"\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    if num_samples == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, ax in zip(indices, axes):\n",
        "            image, target = dataset[idx]\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "            outputs = model(pixel_values=image_tensor)\n",
        "            \n",
        "            logits = outputs.logits[0]\n",
        "            boxes = outputs.pred_boxes[0]\n",
        "            scores = logits.softmax(-1)[:, :-1].max(-1)\n",
        "            labels = scores.indices\n",
        "            scores = scores.values\n",
        "            keep = scores > 0.5\n",
        "            \n",
        "            # Convert image to numpy for visualization\n",
        "            img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = (img_np * 255).astype(np.uint8)\n",
        "            h, w = img_np.shape[:2]\n",
        "            \n",
        "            # Draw bounding boxes\n",
        "            for box, score in zip(boxes[keep], scores[keep]):\n",
        "                cx, cy, bw, bh = box.cpu().numpy()\n",
        "                x1 = int((cx - bw/2) * w)\n",
        "                y1 = int((cy - bh/2) * h)\n",
        "                x2 = int((cx + bw/2) * w)\n",
        "                y2 = int((cy + bh/2) * h)\n",
        "                cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(img_np, f\"{score.item():.2f}\", (x1, y1-5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            \n",
        "            ax.imshow(img_np)\n",
        "            ax.set_title(f\"Predictions (n={keep.sum()})\")\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(student_model, val_dataset, device, num_samples=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary\n",
        "\n",
        "Knowledge distillation training completed successfully!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"🎉 KNOWLEDGE DISTILLATION PIPELINE COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n✓ Configuration loaded from YAML\")\n",
        "print(\"✓ Dataset prepared and loaded\")\n",
        "print(\"✓ Teacher and student models configured\")\n",
        "print(\"✓ Training completed with distillation\")\n",
        "print(\"✓ Model evaluated with COCO metrics\")\n",
        "print(\"✓ Predictions visualized\")\n",
        "print(f\"\\n📁 Output directory: {CONFIG['output_dir']}\")\n",
        "print(\"   - best.pth: Best model checkpoint\")\n",
        "print(\"   - epoch_*.pth: Epoch checkpoints\")\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"   1. Train for more epochs (edit YAML config)\")\n",
        "print(\"   2. Tune hyperparameters in YAML\")\n",
        "print(\"   3. Try different model pairs\")\n",
        "print(\"   4. Deploy the model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from YAML file\n",
        "config_path = Path('configs/distillation.yaml')\n",
        "with open(config_path) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Create flattened CONFIG for easier access\n",
        "CONFIG = {\n",
        "    'kitti_root': './kitti_data/training',\n",
        "    'data_root': config['data']['root'],\n",
        "    'output_dir': './output/distillation_notebook',\n",
        "    'num_labels': config['data']['num_labels'],\n",
        "    'train_split': 0.8,\n",
        "    'max_samples': 200,\n",
        "    'teacher_model': config['model']['teacher'],\n",
        "    'student_model': config['model']['student'],\n",
        "    'batch_size': config['data']['batch_size'],\n",
        "    'num_workers': config['data']['num_workers'],\n",
        "    'epochs': 3,\n",
        "    'learning_rate': config['training']['learning_rate'],\n",
        "    'weight_decay': config['training']['weight_decay'],\n",
        "    'temperature': config['distillation']['temperature'],\n",
        "    'alpha': config['distillation']['alpha'],\n",
        "    'seed': 42,\n",
        "    'device': None,\n",
        "}\n",
        "\n",
        "print(\"📋 Configuration loaded from:\", config_path)\n",
        "print(\"\\n🔧 Notebook overrides (for faster demo):\")\n",
        "print(f\"  • epochs: {config['training']['epochs']} → {CONFIG['epochs']}\")\n",
        "print(f\"  • max_samples: full dataset → {CONFIG['max_samples']}\")\n",
        "print(f\"  • output_dir: {config['output_dir']} → {CONFIG['output_dir']}\")\n",
        "\n",
        "seed_all(CONFIG['seed'])\n",
        "device = get_device(CONFIG['device'])\n",
        "print(f\"\\n🖥️  Using device: {device}\")\n",
        "\n",
        "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\n📊 Active Configuration:\")\n",
        "print(f\"  Teacher: {CONFIG['teacher_model']}\")\n",
        "print(f\"  Student: {CONFIG['student_model']}\")\n",
        "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
        "print(f\"  Alpha: {CONFIG['alpha']}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
