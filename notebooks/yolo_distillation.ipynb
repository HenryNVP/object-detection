{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc76d2c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "    Setup: Install dependencies and import libraries\n",
    "    Data Preparation: Download and convert KITTI to COCO format\n",
    "    Dataset Loading: Create PyTorch datasets\n",
    "    Model Setup: Load teacher and student models\n",
    "    Training: Train with knowledge distillation\n",
    "    Evaluation: Evaluate with COCO metrics\n",
    "    Visualization: Visualize predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c62333",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d84c39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d552f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision transformers pycocotools pillow tqdm pyyaml matplotlib opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4703cb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94388e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'kitti_root': './kitti_data/training',\n",
    "    'data_root': './kitti_yolo',\n",
    "    'output_dir': './output/',\n",
    "    'num_labels': 6,  # Car, Pedestrian, Cyclist, Truck, Tram, Misc (KITTI classes)\n",
    "    'teacher_model': 'facebook/detr-resnet-50',\n",
    "    'student_model': 'facebook/detr-resnet-50',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'epochs_teacher': 30,\n",
    "    'epochs_student': 30,\n",
    "    'learning_rate': 1.0e-4,\n",
    "    'weight_decay': 1.0e-4,\n",
    "    'temperature': 2.0,\n",
    "    'alpha': 0.5,\n",
    "    'seed': 42,\n",
    "    'patience': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f61f9d",
   "metadata": {},
   "source": [
    "### Download KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5959680",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "\n",
    "# URLs\n",
    "KITTI_URLS = {\n",
    "    \"images\": \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip\",\n",
    "    \"labels\": \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip\",\n",
    "    \"calib\": \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip\",\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"/content/kitti_data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Progress bar helper\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "# Download + extract\n",
    "for name, url in KITTI_URLS.items():\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = output_dir / filename\n",
    "\n",
    "    if not filepath.exists():\n",
    "        print(f\"ðŸ“¦ Downloading {name}...\")\n",
    "        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
    "            urlretrieve(url, filename=filepath, reporthook=t.update_to)\n",
    "\n",
    "    print(f\"ðŸ“‚ Extracting {filename}...\")\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "print(\"âœ… KITTI dataset ready at:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb3c2d",
   "metadata": {},
   "source": [
    "### Extract subset of dataset and convert to COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee1226",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "KITTI_CLASS_MAPPING = {\n",
    "    'Car': 'car', 'Van': 'car', 'Truck': 'truck',\n",
    "    'Pedestrian': 'person', 'Person_sitting': 'person',\n",
    "    'Cyclist': 'bicycle', 'Tram': 'train', 'Misc': 'other'\n",
    "}\n",
    "\n",
    "KITTI_CATEGORIES = [\n",
    "    {'id': 1, 'name': 'person'},\n",
    "    {'id': 2, 'name': 'car'},\n",
    "    {'id': 3, 'name': 'truck'},\n",
    "    {'id': 4, 'name': 'bicycle'},\n",
    "    {'id': 5, 'name': 'train'},\n",
    "    {'id': 6, 'name': 'other'},\n",
    "]\n",
    "\n",
    "CAT_NAME_TO_ID = {cat[\"name\"]: cat[\"id\"] for cat in KITTI_CATEGORIES}\n",
    "\n",
    "def read_kitti_label(label_path: Path):\n",
    "    objs = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 15:\n",
    "                continue\n",
    "            objs.append({\n",
    "                \"type\": parts[0],\n",
    "                \"bbox\": [float(x) for x in parts[4:8]],\n",
    "            })\n",
    "    return objs\n",
    "\n",
    "def convert_to_coco(kitti_root, output_dir, split, image_ids):\n",
    "    image_dir, label_dir = kitti_root / \"image_2\", kitti_root / \"label_2\"\n",
    "    out_img_dir = output_dir / \"images\" / split\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images, annotations = [], []\n",
    "    ann_id = 1\n",
    "    for img_idx, img_name in enumerate(tqdm(image_ids, desc=f\"{split}\")):\n",
    "        img_path = image_dir / f\"{img_name}.png\"\n",
    "        if not img_path.exists(): continue\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        shutil.copy(img_path, out_img_dir / f\"{img_name}.png\")\n",
    "        images.append({\"id\": img_idx+1, \"file_name\": f\"{img_name}.png\", \"width\": width, \"height\": height})\n",
    "\n",
    "        label_path = label_dir / f\"{img_name}.txt\"\n",
    "        if not label_path.exists(): continue\n",
    "        for obj in read_kitti_label(label_path):\n",
    "            if obj[\"type\"] not in KITTI_CLASS_MAPPING: continue\n",
    "            name = KITTI_CLASS_MAPPING[obj[\"type\"]]\n",
    "            cat_id = CAT_NAME_TO_ID[name]\n",
    "            x1, y1, x2, y2 = obj[\"bbox\"]\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            if w <= 0 or h <= 0: continue\n",
    "            annotations.append({\n",
    "                \"id\": ann_id, \"image_id\": img_idx+1,\n",
    "                \"category_id\": cat_id, \"bbox\": [x1, y1, w, h],\n",
    "                \"area\": w*h, \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "    return {\"images\": images, \"annotations\": annotations, \"categories\": KITTI_CATEGORIES}\n",
    "\n",
    "# ---- MAIN ----\n",
    "random.seed(seed)\n",
    "image_files = sorted(list((kitti_root / \"image_2\").glob(\"*.png\")))\n",
    "image_ids = [f.stem for f in image_files]\n",
    "if max_samples: image_ids = image_ids[:max_samples]\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "n = len(image_ids)\n",
    "train_end = int(n * train_split)\n",
    "val_end = int(n * (train_split + val_split))\n",
    "splits = {\"train\": image_ids[:train_end], \"val\": image_ids[train_end:val_end], \"test\": image_ids[val_end:]}\n",
    "\n",
    "ann_dir = output_dir / \"annotations\"\n",
    "ann_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for split, ids in splits.items():\n",
    "    coco = convert_to_coco(kitti_root, output_dir, split, ids)\n",
    "    with open(ann_dir / f\"instances_{split}.json\", \"w\") as f:\n",
    "        json.dump(coco, f)\n",
    "    print(f\"âœ… {split}: {len(coco['images'])} images, {len(coco['annotations'])} annotations\")\n",
    "\n",
    "print(\"\\nâœ… Conversion complete! Output:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff55939",
   "metadata": {},
   "source": [
    "## 2.  Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8d619",
   "metadata": {},
   "source": [
    "### Finetune Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763a3b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_teacher(data_yaml_path):\n",
    "    \"\"\"Train teacher YOLOv8m.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: Training Teacher YOLOv8m\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model = YOLO('yolov8m.pt')\n",
    "    \n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=CONFIG['epochs_teacher'],\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        batch=CONFIG['batch_size'] // 2,  # Larger model needs more memory\n",
    "        patience=CONFIG['patience'],\n",
    "        device=device,\n",
    "        project=CONFIG['output_dir'],\n",
    "        name='teacher_yolov8m',\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path,\n",
    "        split='test',\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        batch=CONFIG['batch_size'] // 2,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Teacher Results:\")\n",
    "    print(f\"  mAP@0.50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"  mAP@0.50-0.95: {metrics.box.map:.4f}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fdfe94",
   "metadata": {},
   "source": [
    "### Finetune Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4c1e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_baseline(data_yaml_path):\n",
    "    \"\"\"Train baseline YOLOv8n without distillation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Training Baseline YOLOv8n (no distillation)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    results = model.train(\n",
    "        data=data_yaml_path,\n",
    "        epochs=CONFIG['epochs_student'],\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        batch=CONFIG['batch_size'],\n",
    "        patience=CONFIG['patience'],\n",
    "        device=device,\n",
    "        project=CONFIG['output_dir'],\n",
    "        name='baseline_yolov8n',\n",
    "        exist_ok=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path,\n",
    "        split='test',\n",
    "        imgsz=CONFIG['img_size'],\n",
    "        batch=CONFIG['batch_size'],\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Baseline Results:\")\n",
    "    print(f\"  mAP@0.50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"  mAP@0.50-0.95: {metrics.box.map:.4f}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a6a3a",
   "metadata": {},
   "source": [
    "## 3.  Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da6134",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Distillation loss\n",
    "kl_div = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, T=2.0):\n",
    "    \"\"\"KL-divergence based distillation\"\"\"\n",
    "    s = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    t = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    return kl_div(s, t) * (T * T)\n",
    "\n",
    "# Dataloader from YOLO\n",
    "train_loader = student.model.dataloaders(data_yaml_path, batch_size=batch_size, mode=\"train\")\n",
    "\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"ðŸš€ Starting YOLOv8 â†’ YOLOv8 distillation...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    student.train()\n",
    "    total_loss = 0\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward passes\n",
    "        with torch.no_grad():\n",
    "            teacher_preds = teacher(imgs).logits  # teacher outputs\n",
    "        student_preds = student(imgs).logits\n",
    "\n",
    "        # Compute losses\n",
    "        loss_student = bce_loss(student_preds, torch.zeros_like(student_preds))  # placeholder hard loss\n",
    "        loss_distill = distillation_loss(student_preds, teacher_preds, T=temperature)\n",
    "\n",
    "        loss = alpha * loss_student + (1 - alpha) * loss_distill\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"âœ… Distillation finished!\")\n",
    "student.save(\"student_yolo_distilled.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1899f",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174995d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compare_results(baseline_metrics, teacher_metrics, student_metrics):\n",
    "    \"\"\"Compare all three models.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Comparing Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results_data = {\n",
    "        'Model': ['Baseline YOLOv8n', 'Teacher YOLOv8m', 'Student YOLOv8n (Distilled)'],\n",
    "        'Parameters': ['3.2M', '25.9M', '3.2M'],\n",
    "        'mAP@0.50': [\n",
    "            baseline_metrics.box.map50,\n",
    "            teacher_metrics.box.map50,\n",
    "            student_metrics.box.map50\n",
    "        ],\n",
    "        'mAP@0.50-0.95': [\n",
    "            baseline_metrics.box.map,\n",
    "            teacher_metrics.box.map,\n",
    "            student_metrics.box.map\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(results_data)\n",
    "    \n",
    "    print(\"\\nðŸ“Š FINAL RESULTS:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvement\n",
    "    baseline_map = baseline_metrics.box.map50\n",
    "    student_map = student_metrics.box.map50\n",
    "    improvement = ((student_map - baseline_map) / baseline_map) * 100\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Distillation Improvement: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    models = ['Baseline\\nYOLOv8n', 'Teacher\\nYOLOv8m', 'Student\\nYOLOv8n\\n(Distilled)']\n",
    "    map50_scores = results_data['mAP@0.50']\n",
    "    map_scores = results_data['mAP@0.50-0.95']\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    \n",
    "    axes[0].bar(models, map50_scores, color=colors, alpha=0.7)\n",
    "    axes[0].set_ylabel('mAP@0.50', fontsize=12)\n",
    "    axes[0].set_title('Detection Performance (IoU=0.50)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylim(0, max(map50_scores) * 1.2)\n",
    "    for i, v in enumerate(map50_scores):\n",
    "        axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1].bar(models, map_scores, color=colors, alpha=0.7)\n",
    "    axes[1].set_ylabel('mAP@0.50-0.95', fontsize=12)\n",
    "    axes[1].set_title('Detection Performance (IoU=0.50-0.95)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylim(0, max(map_scores) * 1.2)\n",
    "    for i, v in enumerate(map_scores):\n",
    "        axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_path = Path(CONFIG['output_dir']) / 'comparison.png'\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nðŸ’¾ Comparison plot saved to: {output_path}\")\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
